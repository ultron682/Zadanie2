# Zadanie 2 - Full-Stack Application w Minikube

## A. Opis aplikacji

Wybrana aplikacja to **CodeShare** - platforma do wspÃ³Å‚dzielenia kodu w czasie rzeczywistym. Aplikacja umoÅ¼liwia tworzenie snippetÃ³w kodu, ich edycjÄ™ oraz udostÄ™pnianie poprzez unikalne linki. Jest to moja aplikacja, ktÃ³rÄ… kiedyÅ› stworzyÅ‚em do swojego portfolio. W czasie robienia zadania znowu miaÅ‚em problemy z RAM'em. MusiaÅ‚em trochÄ™ ograniczyÄ‡ zasoby, ale ostatecznie wszystko siÄ™ udaÅ‚o.

PoniÅ¼ej linki prowadzÄ…ce do moich repo. SÄ… tam screenshoty i gify prezentujÄ…ce aplikacjÄ™:
Backend: https://github.com/ultron682/asp.net-ef-xunit-identity_codeshare_backend
Frontend: https://github.com/ultron682/react-signalR_codeshare_frontend

### Stack technologiczny

Aplikacja wykorzystuje stack **MERN-like**:

- **Frontend**: React 18 z SignalR Client, CodeMirror (edytor kodu), React Router
- **Backend**: ASP.NET Core 8.0 Web API z SignalR Hub
- **Baza danych**: Entity Framework Core InMemory (dla uproszczenia w Å›rodowisku Minikube)

## B. Konfiguracja klastra Minikube

Uruchomienie klastra z 4 wÄ™zÅ‚ami i addonem Ingress:

```
minikube start --nodes 4 --cni calico --driver docker
```

WÅ‚Ä…czenie addonu Ingress:

```
minikube addons enable ingress
```

Wynik:

```
ğŸ’¡  ingress is an addon maintained by Kubernetes.
    â–ª Using image registry.k8s.io/ingress-nginx/controller:v1.13.2
ğŸŒŸ  The 'ingress' addon is enabled
```

Sprawdzenie wÄ™zÅ‚Ã³w:

```
kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
minikube       Ready    control-plane   50m   v1.34.0
minikube-m02   Ready    <none>          49m   v1.34.0
minikube-m03   Ready    <none>          48m   v1.34.0
minikube-m04   Ready    <none>          47m   v1.34.0
```

## C. Budowanie obrazÃ³w Docker

### Backend (ASP.NET Core)

```
cd codeshare_backend/asp.net-ef-xunit-identity_codeshare_backend-master
docker build -t codeshare-backend:v1 .
```

Wynik:

```
[+] Building 34.6s (18/18) FINISHED
 => [build 4/7] RUN dotnet restore "CodeShareBackend/CodeShareBackend.csproj"   23.3s
 => [build 7/7] RUN dotnet build "CodeShareBackend.csproj" -c Release            6.3s
 => [publish 1/1] RUN dotnet publish "CodeShareBackend.csproj" -c Release        3.1s
 => naming to docker.io/library/codeshare-backend:v1
```

### Frontend (React)

```
cd react_frontend/react-signalR_codeshare_frontend-master
docker build --build-arg REACT_APP_API_URL=http://brilliantapp.zad/api \
             --build-arg REACT_APP_SIGNALR_URL=http://brilliantapp.zad/codeshare \
             -t codeshare-frontend:v1 .
```

Wynik:

```
[+] Building 128.9s (15/15) FINISHED
 => [build 4/6] RUN npm install                                                 56.7s
 => [build 6/6] RUN npm run build                                               51.9s
 => naming to docker.io/library/codeshare-frontend:v1
```

### ZaÅ‚adowanie obrazÃ³w do Minikube

```
minikube image load codeshare-backend:v1
minikube image load codeshare-frontend:v1
```

## D. Pliki konfiguracyjne wdroÅ¼enia

### Struktura manifestÃ³w

```
manifests/
â”œâ”€â”€ 01-namespaces.yaml       # Namespace codeshare
â”œâ”€â”€ 02-configmaps.yaml       # ConfigMaps dla frontend i backend
â”œâ”€â”€ 03-secrets.yaml          # Secrets (JWT, connection strings)
â”œâ”€â”€ 05-backend-deployment.yaml   # Deployment i Service backend
â”œâ”€â”€ 06-frontend-deployment.yaml  # Deployment i Service frontend
â””â”€â”€ 07-ingress.yaml          # Ingress z routingiem
```

## E. WdroÅ¼enie aplikacji

Aplikacja manifestÃ³w:

```
kubectl apply -f manifests/
```

Wynik:

```
namespace/codeshare created
configmap/frontend-config created
configmap/backend-config created
secret/postgres-secret created
secret/backend-secret created
deployment.apps/backend created
service/backend-service created
deployment.apps/frontend created
service/frontend-service created
ingress.networking.k8s.io/codeshare-ingress created
```

### Weryfikacja Pod-Ã³w

```
kubectl get pods -n codeshare -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE
backend-7db984bf7f-cl6ms    1/1     Running   0          3m15s   10.244.205.216   minikube-m02
backend-7db984bf7f-slwg5    1/1     Running   0          3m15s   10.244.151.20    minikube-m03
frontend-858574987c-ctptv   1/1     Running   0          16m     10.244.59.150    minikube-m04
frontend-858574987c-ffq9q   1/1     Running   0          16m     10.244.205.215   minikube-m02
```

### Weryfikacja serwisÃ³w

```
kubectl get svc -n codeshare
NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
backend-service    ClusterIP   10.109.171.130   <none>        5555/TCP   16m
frontend-service   ClusterIP   10.100.162.54    <none>        80/TCP     16m
```

### Weryfikacja Ingress

```
kubectl get ingress -n codeshare
NAME                CLASS   HOSTS              ADDRESS        PORTS   AGE
codeshare-ingress   nginx   brilliantapp.zad   192.168.49.2   80      16m
```

## F. Konfiguracja dostÄ™pu

### 1. Dodanie wpisu do pliku hosts

Å»eby zadziaÅ‚aÅ‚o musiaÅ‚em do pliku hosts w `C:\Windows\System32\drivers\etc\hosts` dodaÄ‡ odwoÅ‚anie do brilliantapp.zad :

```
127.0.0.1 brilliantapp.zad
```

### 2. Uruchomienie tunelu Minikube

```
minikube tunnel
```

Wynik:

```
âœ…  Tunnel successfully started
ğŸ“Œ  NOTE: Please do not close this terminal as this process must stay alive...
ğŸƒ  Starting tunnel for service codeshare-ingress.
```

### 3. DostÄ™p do aplikacji

Aplikacja dostÄ™pna pod adresem: **http://brilliantapp.zad**

## G. Testy poprawnoÅ›ci dziaÅ‚ania

### Test dostÄ™pnoÅ›ci frontendu

```
kubectl port-forward svc/frontend-service 80:80 -n codeshare &
curl http://localhost:80 | Select-String "title"
```

Wynik:

```
<title>CodeShare</title>
```

### Test dostÄ™pnoÅ›ci backendu (SignalR negotiate)

```
kubectl port-forward svc/backend-service 5555:5555 -n codeshare &
Invoke-WebRequest -Uri http://localhost:5555/codesharehub/negotiate -Method POST
```

Wynik:

```
StatusCode        : 200
StatusDescription : OK
```

Backend SignalR Hub odpowiada poprawnie.

### Test logÃ³w backendu

```
kubectl logs -n codeshare deployment/backend --tail=10
```

Wynik:

```
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://[::]:5555
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
info: Microsoft.Hosting.Lifetime[0]
      Hosting environment: Production
info: Microsoft.Hosting.Lifetime[0]
      Content root path: /app
```

---

# CZÄ˜ÅšÄ† NIEOBOWIÄ„ZKOWA

## A. Aktualizacja aplikacji bez przerywania dziaÅ‚ania

### Opis zmian

Zmiana widoczna po aktualizacji: **zmiana tytuÅ‚u strony** z "Code Share" na "CodeShare v2 - Live Code Editor".

### 1. Modyfikacja pliku index.html

```html
<!-- Przed -->
<title>Code Share</title>

<!-- Po -->
<title>CodeShare v2 - Live Code Editor</title>
```

### 2. Budowanie nowej wersji obrazu

```
cd react_frontend/react-signalR_codeshare_frontend-master
docker build --build-arg REACT_APP_API_URL=http://brilliantapp.zad/api \
             --build-arg REACT_APP_SIGNALR_URL=http://brilliantapp.zad/codeshare \
             -t codeshare-frontend:v2 .
```

Wynik:

```
[+] Building 47.9s (15/15) FINISHED
 => [build 6/6] RUN npm run build                                               42.6s
 => naming to docker.io/library/codeshare-frontend:v2
```

ZaÅ‚adowanie do Minikube:

```
minikube image load codeshare-frontend:v2
```

### 3. Aktualizacja Deployment

```
kubectl set image deployment/frontend frontend=docker.io/library/codeshare-frontend:v2 -n codeshare
```

Wynik:

```
deployment.apps/frontend image updated
```

### 4. Monitorowanie rolling update

```
kubectl rollout status deployment/frontend -n codeshare
```

Wynik:

```
Waiting for deployment "frontend" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "frontend" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "frontend" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "frontend" rollout to finish: 1 of 2 updated replicas are available...
deployment "frontend" successfully rolled out
```

### 5. Weryfikacja

Stan Pod-Ã³w po aktualizacji:

```
kubectl get pods -n codeshare -o wide
NAME                        READY   STATUS    AGE     NODE
backend-7db984bf7f-cl6ms    1/1     Running   6m41s   minikube-m02
backend-7db984bf7f-slwg5    1/1     Running   6m41s   minikube-m03
frontend-766596b6d6-kfm6z   1/1     Running   16s     minikube-m03
frontend-766596b6d6-xff6m   1/1     Running   16s     minikube-m04
```

Test nowego tytuÅ‚u:

```
kubectl port-forward svc/frontend-service 80:80 -n codeshare &
curl http://localhost:80 | Select-String "title"
```

Wynik:

```
<title>CodeShare v2 - Live Code Editor</title>
```

Zmiana zostaÅ‚a wdroÅ¼ona bez przerywania dostÄ™pnoÅ›ci aplikacji. Czyli aktualizacja udana.

## B. Uzasadnienie konfiguracji sond i uzasadnienie doboru

### Sondy TCP Socket

WybraÅ‚em sondy typu `tcpSocket` zamiast `httpGet` poniewaÅ¼:

1. **startupProbe** - sprawdza czy aplikacja ASP.NET Core zdÄ…Å¼yÅ‚a siÄ™ uruchomiÄ‡. Ustawienia:

   - `initialDelaySeconds: 5` - daje czas na bootstrap kontenera
   - `periodSeconds: 3` - szybkie sprawdzanie
   - `failureThreshold: 20` - Å‚Ä…cznie 65 sekund na uruchomienie (5 + 20\*3)

2. **readinessProbe** - sprawdza czy aplikacja jest gotowa do przyjmowania ruchu:

   - `initialDelaySeconds: 10` - po startupProbe
   - `periodSeconds: 5` - regularne sprawdzanie

3. **livenessProbe** - sprawdza czy aplikacja nadal dziaÅ‚a:

   - `initialDelaySeconds: 15` - dÅ‚uÅ¼szy czas poczÄ…tkowy
   - `periodSeconds: 10` - rzadsze sprawdzanie
   - `failureThreshold: 3` - restart po 3 nieudanych prÃ³bach

### Dlaczego wybraÅ‚em TCP zamiast HTTP?

- No bo w mojej aplikacji Swagger UI (czyli taka stronka do testowania endpointÃ³w) jest dostÄ™pna tylko w trybie Development
- Nie dodaÅ‚em wtedy do aplikacji dedykowanego health endpointu
- I najwaÅ¼niejsze: TCP Socket skutecznie weryfikuje czy port nasÅ‚uchuje

### Implementacja wszystkich trzech typÃ³w sond

Backend wykorzystuje wszystkie 3 typy sond:

```yaml
startupProbe:
  tcpSocket:
    port: 5555
  initialDelaySeconds: 5
  periodSeconds: 3
  timeoutSeconds: 2
  failureThreshold: 20

readinessProbe:
  tcpSocket:
    port: 5555
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

livenessProbe:
  tcpSocket:
    port: 5555
  initialDelaySeconds: 15
  periodSeconds: 10
  timeoutSeconds: 3
  failureThreshold: 3
```

### WspÃ³Å‚praca sond podczas rolling update

1. Nowy Pod startuje â†’ startupProbe sprawdza uruchomienie
2. startupProbe przechodzi â†’ readinessProbe sprawdza gotowoÅ›Ä‡
3. readinessProbe przechodzi â†’ Pod dodany do Service endpoints
4. Stary Pod otrzymuje SIGTERM â†’ graceful shutdown
5. livenessProbe monitoruje nowe Pod-y pod kÄ…tem zawieszenia

Ta konfiguracja wedÅ‚ug mnie powinnma zagwarantowaÄ‡ **zero downtime deployment**.
Mam nadziejÄ™, Å¼e udaÅ‚o mi siÄ™ w peÅ‚ni wykonaÄ‡ zadanie.
